llm = "claude-3-haiku-20240307"

concepts: dict = {
    "Neural Network Hyper-parameters": [
        "Learning rate",
        "Batch size",
        "Number of epochs",
        "Optimizer choice (Adam, SGD, RMSprop)",
        "Regularization strength (L1, L2)",
        "Dropout rate",
        "Number of hidden layers",
        "Number of neurons per layer",
        "Activation functions",
        "Early stopping patience",
        "Learning rate decay/schedule",
        "Momentum",
        "Weight initialization method",
        "Kernel size (for CNNs)",
        "Number of filters (for CNNs)",
        "Stride and padding (for CNNs)",
        "Embedding dimension (for NLP tasks)",
        "Number of attention heads (for transformer models)",
        "Warmup steps (for transformer models)",
        "Tree depth (for decision trees and random forests)",
    ],
    "Neural Network Optimizer": [
        "Optimisation Target (Loss)",
        "Optimisation algorithm (Gradient descent, RMSProp, Adam, AdamW)",
        "Local minima",
        "Momentum",
        "Normalization",
        "Stochastic",
        "Batching",
        "Over-fitting",
    ],
    "Neural Network Adversarial attack": [
        "Robustness",
        "Internal concepts",
        "White-box / Black-box",
        "Targeted / Untargeted",
        "Fast Gradient Sign Method",
        "Adversarial patches",
        "Transferability",
        "Adversarial training",
        "Defensive distillation",
        "Softmax",
        "Prompt injection",
        "Critical application",
    ],
    "AI Agents": [
        "Scaffolding",
        "Memory / Planning / Prompt / Knowledge / Tools",
        "Chain of thoughts, Tree of thoughts",
    ],
    "AI Evals": [
        "Policy making",
        "Preparedness / RSP / Safety Framework",
        "Safety washing",
        "False sense of safety",
        "Deceptiveness",
        "METR",
        "Situational awareness",
        "Utility",
        "Power aversion",
        "Reward",
        "Morality",
    ],
    "Transformers": [
        "Softmax",
        "Logits",
        "Tokenizer",
        "Sampler",
        "Residual Stream",
        "Residual connection",
        "Embedding, Positional Embedding",
        "Layer Norm",
        "Attention block",
        "MLP block",
        "Un-embedding",
    ],
    "AI Alignment threat model": [
        "Specification gaming",
        "Goal misgeneralization",
        "Multi-agent interaction",
        "Misaligned power seeking",
        "Pre-training",
        "Human Feedback on Diverse Tasks (HFDT)",
        "Racing Forward",
        "Naive safety efforts",
        "Mesa-Optimisation",
        "Inner alignment",
        "Outer alignment",
        "Deceptive alignment",
    ],
    "AI Red teaming": [
        "Unauthorized access",
        "Harmful use",
        "Improper output handling",
        "Prompt injection",
        "Unintended propensities",
        "Model stealing",
        "DDOS",
        "Bad Integration",
        "Jailbreak",
        "White-box / Black-box",
        "Affirmative suffixes",
        "Refusal suppression",
        "Context switching",
        "In-context learning",
        "Input euphemisms",
        "Virtual simulation",
        "Ciphers",
        "Personification",
        "Code evaluation",
        "Inductive challenge",
        "Tensor Trust",
    ],
    "Neural Network Reinforcement Learning (RL)": [
        "State (Markov)",
        "Actions",
        "Reward",
        "Policy (Optimal policy)",
        "Trajectory",
        "Value function",
        "Bellman equation",
        "Action value function Q",
        "Deep Q learning Network",
        "Temporal Difference Learning",
        "Îµ-greedy policy",
        "A2C (actor critic)",
    ],
    "AI Governance best practice": [
        "Third party model audit",
        "Safety restriction",
        "Military grade information security",
        "KYC screening",
        "Model containment",
        "No Open-Sourcing",
        "Researcher model access",
        "Pre-registration of bug training runs",
        "Notify affected parties",
        "Inter-lab scrutiny",
        "Avoid capability jumps",
        "Notify other labs",
        "Military supervision",
        "International scientific collaboration",
    ],
    "AI Safety Strategy": [
        "Awareness",
        "Fundraising",
        "Alignment Research",
        "Governance",
        "Forecasting",
        "Field building and up-skilling",
    ],
    "RLHF (Reinforcement Learning from Human Feedback)": [
        "Fine-tune models",
        "KL divergence",
        "Constitutional AI",
        "Direct Preference Optimization",
    ],
    "Neural Network Interpretability (Interp)": [
        "Mind reading",
        "Eval from inside",
        "Precise control",
        "Induction heads",
        "LogitLens",
        "Steering Vectors",
        "Micro / Macro Circuit Analysis",
        "SAE (Sparse Autoencoder)",
        "Dimensionality reduction",
        "Data Compression",
    ],
    "Meta skills": [
        "Learning in public",
        "Getting things done",
        "Cognitive Behavioural Theory",
        "Exercise and health",
        "Taking initiative",
    ],
    "Theory of Change": [
        "Specificity",
        "Help",
        "Timeliness",
        "Feasibility",
        "Impact",
        "Unique Strengths",
        "Cost",
    ],
    "Additional Topics": [
        "Capabilities",
        "Risks Landscape",
        "Solutions Landscape",
    ],
}
